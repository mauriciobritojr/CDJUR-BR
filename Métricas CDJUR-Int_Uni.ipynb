{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1475af27",
   "metadata": {},
   "source": [
    "### Gera Coleção Dourada (.json) e planilha de métricas Kappa - Última versão\n",
    "Última atualização em 16/12 - Geração da CDJUR - TJ após 2a. revisão"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d7b036",
   "metadata": {},
   "source": [
    "#### Função para checar se há interseção entre dois intervalos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7028c37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOverlap(a, b):\n",
    "    overlap = max(0, min(a[1], b[1]) - max(a[0], b[0]))\n",
    "    if overlap > 0:\n",
    "        if a[0] < b[0]:\n",
    "            return True #, 0\n",
    "        else:\n",
    "            return True # , 1\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# inte = getOverlap([565, 667],[530, 567])\n",
    "\n",
    "# inte =  getOverlap([10, 15], [20, 38])\n",
    "# print (inte)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5108170",
   "metadata": {},
   "source": [
    "#### Função para Extrair a Interseção entre duas Sentenças"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4def85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para realizar a interseção entre sentenças\n",
    "def intersecao_string(X, Y):\n",
    "    X = ' '.join(X.split())\n",
    "    Y = ' '.join(Y.split())\n",
    "    list_X = X.split(' ')\n",
    "    list_Y = Y.split(' ')\n",
    "    intersecao = ''\n",
    "    i = 0\n",
    "    for token in list_X:\n",
    "        if token in list_Y:\n",
    "            if i > 0:\n",
    "                intersecao += ' '\n",
    "            intersecao += token\n",
    "        i += 1\n",
    "    return intersecao.lower() # retorna a interseção em letras mínúsculas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9bd9f0",
   "metadata": {},
   "source": [
    "#### Finção para fazer a União entre duas sentenças"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2f36be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para realizar a união entre sentenças\n",
    "def uniao_string(tupla_ann1, tupla_ann2):\n",
    "    uniao = ''\n",
    "    lst_sentenca1 = []\n",
    "    lst_sentenca2 = []\n",
    "    \n",
    "    ann1 = {'inicio':tupla_ann1[1][0],'fim':tupla_ann1[1][1]}\n",
    "    ann2 = {'inicio':tupla_ann2[1][0],'fim':tupla_ann2[1][1]}\n",
    "    \n",
    "    # determina o ínicio da sentença\n",
    "    if ann1['inicio'] <= ann2['inicio']:\n",
    "        inicio = ann1['inicio']\n",
    "        sentenca1 = tupla_ann1[2]\n",
    "        sentenca2 = tupla_ann2[2]\n",
    "    else:\n",
    "        inicio = ann2['inicio']\n",
    "        sentenca1 = tupla_ann2[2]\n",
    "        sentenca2 = tupla_ann1[2]\n",
    "            \n",
    "    # determina o fim da sentença\n",
    "    if ann1['fim'] >= ann2['fim']:\n",
    "        fim = ann1['fim']\n",
    "    else:\n",
    "        fim = ann2['fim']\n",
    "    \n",
    "    # verifica se uma sentença está contida na outra\n",
    "    if ((ann1['fim'] <= ann2['fim']) and (ann1['inicio'] >= ann2['inicio'])):\n",
    "        uniao = tupla_ann2[2]\n",
    "        return inicio, fim, uniao.lower()\n",
    "    else:\n",
    "        if ((ann2['fim'] <= ann1['fim']) and (ann2['inicio'] >= ann1['inicio'])):\n",
    "            uniao = tupla_ann1[2]\n",
    "            return inicio, fim, uniao.lower()\n",
    "        else:\n",
    "            # Para o caso das sentenças não estarem contidas uma na outra    \n",
    "            comprimento = fim - inicio\n",
    "            diferenca   = abs(ann1['inicio'] - ann2['inicio'])\n",
    "            \n",
    "            idx = 0 - diferenca\n",
    "            for i in range(comprimento):\n",
    "                if i < len(sentenca1):\n",
    "                    lst_sentenca1.append(sentenca1[i])\n",
    "                else:\n",
    "                    lst_sentenca1.append('~')\n",
    "                    \n",
    "                if i >= diferenca:\n",
    "                    lst_sentenca2.append(sentenca2[idx])\n",
    "                else:\n",
    "                    lst_sentenca2.append('~')\n",
    "                \n",
    "                idx += 1\n",
    "\n",
    "            for i in range(comprimento):\n",
    "                if lst_sentenca1[i] != '~' and i < len(sentenca1):\n",
    "                    uniao += lst_sentenca1[i]\n",
    "                \n",
    "                if i >= len(sentenca1) and lst_sentenca2[i] != '~':\n",
    "                    uniao += lst_sentenca2[i]\n",
    "                    \n",
    "            return inicio, fim, uniao.lower() # retorna a interseção em letras mínúsculas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b032f0af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 112, 'iptu/ imposto predial e territorial urbano')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # teste da funçao união\n",
    "ann2 = ('ASSUNTO_CNJ', (70, 112), 'IPTU/ Imposto Predial e Territorial Urbano')\n",
    "# ann1 = ('ASSUNTO_CNJ', (76, 122), 'Imposto Predial e Territorial Urbano nacional.')\n",
    "ann1 = ('ASSUNTO_CNJ', (70, 112), 'IPTU/ Imposto Predial e Territorial Urbano')\n",
    "uniao_string(ann1, ann2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3521f4bd",
   "metadata": {},
   "source": [
    "#### Algoritmo principal de Cálculo das Métricas Kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18f79435",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gerando Coleção Dourada do Tribunal de Justiça.\n",
      "Iniciando primeira etapa.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 1240/1240 [00:00<00:00, 310077.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando segunda etapa.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]C:\\Users\\mbrit\\anaconda3\\envs\\projeto_tj_mp\\lib\\site-packages\\ipykernel_launcher.py:374: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "C:\\Users\\mbrit\\anaconda3\\envs\\projeto_tj_mp\\lib\\site-packages\\ipykernel_launcher.py:375: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "134it [00:10,  7.64it/s]C:\\Users\\mbrit\\anaconda3\\envs\\projeto_tj_mp\\lib\\site-packages\\ipykernel_launcher.py:174: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "C:\\Users\\mbrit\\anaconda3\\envs\\projeto_tj_mp\\lib\\site-packages\\ipykernel_launcher.py:175: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "620it [02:47,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conta lista_anotacao 13407\n",
      "conta appends 13407\n",
      "Quantidade de anotações: 13407\n",
      "Quantidade de anotações discordantes: 2\n",
      "--- 169.77430248260498 segundos ---\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "import math\n",
    "import glob, os, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import xlsxwriter\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "def get_dict_classe_tag(legenda):\n",
    "    dict_classe_tag = dict()\n",
    "    for tag in legenda.values():\n",
    "        classe = tag\n",
    "#         classe = tag.split(\"_\")[0] # código original para criar os grupos de tags\n",
    "        if classe not in dict_classe_tag.keys():\n",
    "            dict_classe_tag[classe] = []\n",
    "        dict_classe_tag[classe].append(tag)\n",
    "\n",
    "    return dict_classe_tag\n",
    "\n",
    "def inicializar_matriz_concordancia(dict_legenda):\n",
    "    labels = []\n",
    "    for chave, conjunto in dict_legenda.items():\n",
    "        if conjunto == []:\n",
    "            labels += [chave]\n",
    "        else:\n",
    "            labels += sorted(conjunto)\n",
    "    labels = sorted(labels) + ['NENHUM']\n",
    "\n",
    "    return pd.DataFrame(index=labels, columns=labels).fillna(0)\n",
    "\n",
    "def combinacao(a, n):\n",
    "    if n == 1:\n",
    "        for x in a:\n",
    "            yield [x]\n",
    "    else:\n",
    "        for i in range(len(a)):\n",
    "            for x in combinacao(a[:i], n-1):\n",
    "                yield [a[i]] + x\n",
    "                \n",
    "\n",
    "def similaridade_sentenca(X, Y):                # Program to measure the similarity between\n",
    "                                              # two sentences using cosine similarity.\n",
    "    # tokenization\n",
    "    X_list = word_tokenize(X)\n",
    "    X_list = [token.lower() for token in X_list]\n",
    "    Y_list = word_tokenize(Y)\n",
    "    Y_list = [token.lower() for token in Y_list]\n",
    "\n",
    "    # sw contains the list of stopwords\n",
    "    sw = stopwords.words('portuguese') \n",
    "    l1 =[];l2 =[]\n",
    "\n",
    "    # remove stop words from the string\n",
    "    X_set = {w for w in X_list if not w in sw} \n",
    "    Y_set = {w for w in Y_list if not w in sw}\n",
    "\n",
    "    # form a set containing keywords of both strings \n",
    "    rvector = X_set.union(Y_set) \n",
    "    for w in rvector:\n",
    "        if w in X_set: l1.append(1) # create a vector\n",
    "        else: l1.append(0)\n",
    "        if w in Y_set: l2.append(1)\n",
    "        else: l2.append(0)\n",
    "    c = 0\n",
    "\n",
    "    # cosine formula \n",
    "    for i in range(len(rvector)):\n",
    "            c+= l1[i]*l2[i]\n",
    "    cosine = c / float((sum(l1)*sum(l2))**0.5)\n",
    "    \n",
    "    if cosine >= (1 - threshold):\n",
    "        return cosine\n",
    "    else:\n",
    "        return 0              \n",
    "                \n",
    "def similaridade_jaccard(ann1, ann2):\n",
    "\n",
    "    if ann1['inicio'] == -1 and ann2['inicio'] == -1:\n",
    "        return 0\n",
    "    else:\n",
    "        intersecao = min(ann1['fim'], ann2['fim']) - max(ann1['inicio'], ann2['inicio'])\n",
    "        uniao = max(ann1['fim'], ann2['fim']) - min(ann1['inicio'], ann2['inicio'])\n",
    "        return max(intersecao/uniao,0)\n",
    "\n",
    "def similaridade_esta_contido(ann1, ann2):\n",
    "    if ((ann1['fim'] <= ann2['fim']) and (ann1['inicio'] >= ann2['inicio'])) or ((ann2['fim'] <= ann1['fim']) and (ann2['inicio'] >= ann1['inicio'])):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def similaridade_vizinhos(ann1, ann2, dist_max):\n",
    "    fim =  min(ann1['fim'], ann2['fim'])\n",
    "    inicio = max(ann1['inicio'], ann2['inicio'])\n",
    "\n",
    "    if (inicio-fim) <= dist_max:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def medida_kappa(matriz_kappa, conjunto_tags=[]):\n",
    "    p0 = 0  # É a concordância relativa observada entre avaliadores\n",
    "    pe = 0  # É a probabilidade hipotética de concordância casual (\"e\"sperada).\n",
    "    total = 2 * (matriz_kappa.sum().sum())\n",
    "\n",
    "    if len(conjunto_tags) == 0:\n",
    "          tags = intersecao_listas(matriz_kappa.index.values,\n",
    "                             matriz_kappa.columns.values)\n",
    "          tags.remove(\"NENHUM\")\n",
    "\n",
    "    for tag in tags:\n",
    "        p0         += 2 * matriz_kappa.loc[tag, tag]     \n",
    "        soma_linha  = matriz_kappa.loc[tag, :].sum()\n",
    "        soma_coluna = matriz_kappa.loc[:, tag].sum()\n",
    "        pe         += soma_linha*soma_coluna\n",
    "            \n",
    "    k = 0\n",
    "    p0 = p0 / total\n",
    "    pe = pe / (total*total)\n",
    "    k = (p0-pe)/(1-pe)\n",
    "\n",
    "    return(k)\n",
    "\n",
    "def get_funcao_similaridade(threshold,*funcoes_similaridade):\n",
    "\n",
    "    def funcao_similaridade(tupla_ann1,tupla_ann2):\n",
    "\n",
    "        similaridades = []\n",
    "        ann1 = {'inicio':tupla_ann1[1][0],'fim':tupla_ann1[1][1]}\n",
    "        ann2 = {'inicio':tupla_ann2[1][0],'fim':tupla_ann2[1][1]}\n",
    "        \n",
    "        grupo_ann1 = tupla_ann1[0].split('_')\n",
    "        grupo_ann2 = tupla_ann2[0].split('_')\n",
    "        \n",
    "        if grupo_ann1[0] == grupo_ann2[0]:      # Confere se os grupo da anotação é o mesmo\n",
    "            \n",
    "            if (tupla_ann1[0] == tupla_ann2[0]) and (tupla_ann1[0] == 'CLASSE_CNJ' or tupla_ann1[0] == 'ASSUNTO_CNJ'):\n",
    "                ann1 = tupla_ann1[2]\n",
    "                ann2 = tupla_ann2[2]\n",
    "                similaridades.append(similaridade_sentenca(ann1,ann2))\n",
    "\n",
    "                if max(similaridades) >= threshold:\n",
    "                    return 1\n",
    "                else:\n",
    "                    return 0\n",
    "            \n",
    "            else:\n",
    "                if (tupla_ann1[0] == tupla_ann2[0]):\n",
    "                    lista_funcoes = [similaridade_jaccard, similaridade_esta_contido]\n",
    "                    similaridades  = [f(ann1, ann2) for f in funcoes_similaridade ]\n",
    "                    if max(similaridades) >= threshold:\n",
    "                        return 1\n",
    "                    else:\n",
    "                        return 0\n",
    "                else:\n",
    "                    return float('nan')\n",
    "\n",
    "        else:\n",
    "            return float(\"nan\")\n",
    "        \n",
    "    return funcao_similaridade\n",
    "\n",
    "def intersecao_listas(lst1, lst2):\n",
    "    lst3 = [value for value in lst1 if value in lst2]\n",
    "    return lst3\n",
    "\n",
    "def listas_anotacoes_para_matriz_concordancia(lista_anotacoes_1,lista_anotacoes_2,legenda):\n",
    "\n",
    "    matriz_concordancia = inicializar_matriz_concordancia(dict_entidades)\n",
    "\n",
    "    indice = lista_anotacoes_1 if len(lista_anotacoes_1) > 0 else np.array([['nada',(-1,-1)]])\n",
    "    col = lista_anotacoes_2 if len(lista_anotacoes_2) > 0 else np.array([['nada',(-1,-1)]])\n",
    "    matriz_anotacao = pd.DataFrame(index=indice, columns=col).fillna(0)              \n",
    "    df = matriz_anotacao.stack()\n",
    "    \n",
    "    # Lista de tags e anotações para a CDJUR \n",
    "    tag_ann = []\n",
    "    \n",
    "    # Lista de tags discordantes\n",
    "    tag_ann_disc = []\n",
    "    ct_app=0\n",
    "    for linha in df.keys():\n",
    "        un_string = ''\n",
    "        ann1 = linha[0]\n",
    "        ann2 = linha[1]\n",
    "        df.loc[ann1, ann2] = funcao_similaridade(ann1, ann2)\n",
    "\n",
    "#         print('-------------------------------------------------')\n",
    "        \n",
    "        if df.loc[ann1, ann2] == 1 and ann1[0] != 'nada':\n",
    "#             print('**************************** há concordância *************************')\n",
    "#             tag_ann.append((ann1[0], intersecao_string(ann1[2], ann2[2]))) # guarda a anotação por interseção\n",
    "            un_string = uniao_string(ann1, ann2)\n",
    "#             print('ann1', ann1[0], ann1[1][0], ann1[1][1])\n",
    "#             print(tag_ann)\n",
    "            # verifica se há erro de tag repetidas (anotação duplicada)\n",
    "            if  (ann1[0], un_string) not in tag_ann:\n",
    "                if len(tag_ann) == 0:\n",
    "#                     print('tag_ann vazia - append')\n",
    "                    tag_ann.append((ann1[0], un_string)) # guarda a anotação por união\n",
    "                    ct_app += 1\n",
    "                else:\n",
    "                    # confere se há sobreposição com as tag já guardadas\n",
    "                    for annot in tag_ann:\n",
    "                        intersecao = False\n",
    "                        intersecao = getOverlap([ann1[1][0], ann1[1][1]],[annot[1][0], annot[1][1]])\n",
    "                        if intersecao and ann1[0] == annot[0]:\n",
    "#                             print(ann1[0], [ann1[1][0], ann1[1][1]],' - ', [annot[1][0], annot[1][1]])\n",
    "#                             print('ann1', ann1[2])\n",
    "#                             print('annot', annot[1][2])\n",
    "#                             prin\n",
    "                            ann2_aux = (annot[0], (annot[1][0], annot[1][1]),annot[1][2])\n",
    "                            new_annot = (annot[0], (uniao_string(ann1, ann2_aux)))\n",
    "                            annot = new_annot\n",
    "                            break\n",
    "                    \n",
    "                    ####\n",
    "                    if intersecao and ann1[0] == annot[0]:\n",
    "                        ann2_aux = (annot[0], (annot[1][0], annot[1][1]),annot[1][2])\n",
    "                        new_annot = (annot[0], (uniao_string(ann1, ann2_aux)))\n",
    "                        annot = new_annot\n",
    "#                         print('tag repetida revisada',ann1[0], [ann1[1][0], ann1[1][1]],' - ', annot[0],[annot[1][0], annot[1][1]])\n",
    "                    else:\n",
    "                        if ann1[0] == annot[0] and not intersecao:\n",
    "#                             print(intersecao, 'append tag_ann', ann1[0])\n",
    "                            tag_ann.append((ann1[0], un_string)) # guarda a anotação por união\n",
    "                            ct_app += 1\n",
    "                        if ann1[0] != annot[0] and not intersecao:\n",
    "#                             print(intersecao, 'append tag_ann', ann1[0])\n",
    "                            tag_ann.append((ann1[0], un_string)) # guarda a anotação por união\n",
    "                            ct_app += 1\n",
    "                    ####\n",
    "\n",
    "        else:\n",
    "            if df.loc[ann1, ann2] == 0 and ann1[0] != 'nada':\n",
    "                intersecao = False\n",
    "                intersecao = getOverlap([ann1[1][0], ann1[1][1]],[ann2[1][0], ann2[1][1]])\n",
    "                if intersecao:\n",
    "                    # registra anotações discordantes\n",
    "                    un_string = uniao_string(ann1, ann2)\n",
    "                    # verifica se há erro de tag repetidas (anotação duplicada)\n",
    "                    if  (ann1[0], un_string) not in tag_ann:\n",
    "                        tag_ann_disc.append((ann1[0], un_string)) # guarda a anotação por união\n",
    "     \n",
    "    matriz_anotacao = df.unstack()\n",
    "\n",
    "    linha_nenhum = pd.Series(matriz_anotacao.sum(axis=0).apply(lambda x: 0 if x >= 1 or not math.isnan(x) else 1), name='NENHUM')\n",
    "    matriz_anotacao = matriz_anotacao.append(linha_nenhum)\n",
    "    matriz_anotacao['NENHUM'] = matriz_anotacao.sum(axis=1).apply(lambda x: 0 if x>=1 else 1)\n",
    "    matriz_anotacao.loc['NENHUM','NENHUM'] = 0\n",
    "    matriz_anotacao.index = [ind[0] for ind in matriz_anotacao.index[:-1]] + ['NENHUM']\n",
    "    matriz_anotacao.columns = [ind[0] for ind in matriz_anotacao.columns[:-1]] + ['NENHUM']\n",
    "    matriz_anotacao = matriz_anotacao.groupby(matriz_anotacao.columns, axis=1).sum().groupby(matriz_anotacao.index, axis=0).sum()\n",
    "    \n",
    "    matriz_concordancia.update(matriz_anotacao)\n",
    "    \n",
    "    return matriz_concordancia, tag_ann, tag_ann_disc, ct_app\n",
    "\n",
    "def ler_anotacoes(caminho_ann):\n",
    "\n",
    "    lista = []\n",
    "    anotacoes = json.load(open(caminho_ann,'rb'))\n",
    "    \n",
    "    # Inclui informações para as tags Classe e Assunto CNJ (m_68 e m_69)\n",
    "    if len(anotacoes['metas']) == 1:\n",
    "        try:\n",
    "            tag = 'CLASSE_CNJ'\n",
    "            inicio = 1   #apenas uma constante para atender ao algoritmo já existente\n",
    "            texto  = anotacoes['metas']['m_68']['value']\n",
    "            fim = inicio+len(texto)\n",
    "            lista.append((tag,(inicio,fim),texto))\n",
    "        except:\n",
    "            tag = 'ASSUNTO_CNJ'\n",
    "            inicio = 50   #apenas uma constante para atender ao algoritmo já existente\n",
    "            texto  = anotacoes['metas']['m_69']['value']\n",
    "            fim = inicio+len(texto)\n",
    "            lista.append((tag,(inicio,fim),texto))\n",
    "            \n",
    "    if len(anotacoes['metas']) == 2:\n",
    "        tag = 'CLASSE_CNJ'\n",
    "        inicio = 1   #apenas uma constante para atender ao algoritmo já existente\n",
    "        texto  = anotacoes['metas']['m_68']['value']\n",
    "        fim = inicio+len(texto)\n",
    "        lista.append((tag,(inicio,fim),texto))\n",
    "        \n",
    "        tag = 'ASSUNTO_CNJ'\n",
    "        inicio = 50   #apenas uma constante para atender ao algoritmo já existente\n",
    "        texto  = anotacoes['metas']['m_69']['value']\n",
    "        fim = inicio+len(texto)\n",
    "        lista.append((tag,(inicio,fim),texto))\n",
    "\n",
    "\n",
    "    for ann in anotacoes['entities']:\n",
    "        \n",
    "        tag = legenda_cd[ann['classId']]\n",
    "        inicio = ann['offsets'][0]['start']\n",
    "        texto = ann['offsets'][0]['text']\n",
    "        fim = inicio+len(texto)\n",
    "        \n",
    "        lista.append((tag,(inicio,fim),texto))\n",
    "\n",
    "    return lista\n",
    "\n",
    "def get_submatriz_concordancia(matriz,lista_tags):\n",
    "    resultado = matriz.copy()\n",
    "    resultado = resultado.loc[lista_tags+['NENHUM'],lista_tags+['NENHUM']]\n",
    "    \n",
    "\n",
    "    resultado.loc['NENHUM',lista_tags] += matriz.loc[~matriz.index.isin(resultado.index),matriz.columns.isin(lista_tags)].sum(axis=0).loc[lista_tags]\n",
    "    resultado.loc[lista_tags,\"NENHUM\"] += matriz.loc[matriz.index.isin(lista_tags),~matriz.columns.isin(resultado.columns),].sum(axis=1).loc[lista_tags]\n",
    "    return resultado\n",
    "\n",
    "# PRIMEIRA ETAPA - Cria um dataframe dos documentos                                                PRIMEIRA ETAPA\n",
    "def caminho_anotacoes_para_df(caminho_cdjur,legenda):\n",
    "    print('Iniciando primeira etapa.')\n",
    "#     print(caminho_cdjur)\n",
    "    lista_docs = glob.glob(caminho_cdjur + \"/**/*.json\",recursive=True)\n",
    "    lista_documentos = []\n",
    "    for doc_completo in tqdm(lista_docs):\n",
    "        d = {}\n",
    "        doc = doc_completo.replace(caminho_cdjur,\"\").split(os.sep)\n",
    "        anotador = doc[1]\n",
    "#         cddocumento = doc[-1].split(\"_\")[-1].split(\".\")[0] # original\n",
    "        cddocumento = doc[-1].split(\"-\")[-1].split(\".\")[0] # versão com o nome completo do arquivo\n",
    "        d['anotador'] = anotador\n",
    "        d['cddocumento'] = cddocumento\n",
    "        d['caminho'] = doc_completo\n",
    "        lista_documentos.append(d)\n",
    "\n",
    "    dataframe_documentos = pd.DataFrame(lista_documentos).groupby('cddocumento').agg(list).reset_index()\n",
    "    dataframe_documentos['anotacoes'] = dataframe_documentos['caminho'].apply(lambda x: [ler_anotacoes(y) for y in x])\n",
    "    dataframe_documentos.to_csv('df_docs'+sufixo_file+'.csv', sep=',', encoding='utf-8') #Dataframe de ocorrência de tags\n",
    "    dataframe_documentos = dataframe_documentos.drop('caminho',axis=1)\n",
    "    lista_matrizes   = []\n",
    "    lista_cdjur      = []\n",
    "    lista_cdjur_disc = []\n",
    "    dicionario_cdjur      = {} # Cria o dicionário final da cdjur por documento\n",
    "    dicionario_cdjur_disc = {} # Cria o dicionário final da cdjur por documento - discordante\n",
    "\n",
    "    print('Iniciando segunda etapa.')\n",
    "    conta_append = 0\n",
    "    conta = 0\n",
    "    ct_tag = 0\n",
    "    for documento in tqdm(dataframe_documentos.itertuples(index=True)):\n",
    "        dict_doc        = {}\n",
    "        dict_doc_disc   = {}\n",
    "        lista_dict      = []\n",
    "        lista_dict_disc = []\n",
    "        # Lista de tag_anotação de cada documento\n",
    "        tag_anotacao   = []\n",
    "        # Lista de anotações discordantes\n",
    "        tag_anotacao_disc   = []\n",
    "        anotadores = documento.anotador\n",
    "\n",
    "        if len(anotadores) > 2:\n",
    "            print(documento.cddocumento)\n",
    "            print(\"Nr Par anotadores:\", anotadores)\n",
    "            print(anotadores[0], anotadores[1])\n",
    "            anotadores = anotadores[:-1]\n",
    "        \n",
    "        par_anotadores = combinacao(anotadores,2)\n",
    "        dict_matriz_doc = dict()\n",
    "            \n",
    "        for anotador1,anotador2 in par_anotadores:\n",
    "            dict_matriz_doc = dict()\n",
    "            dict_matriz_doc['cddocumento'] = documento.cddocumento\n",
    "            dict_matriz_doc['anotador1'] = anotador1\n",
    "            dict_matriz_doc['anotador2'] = anotador2\n",
    "            id1, = np.where(np.array(anotadores) == anotador1)[0]\n",
    "            id2, = np.where(np.array(anotadores) == anotador2)[0]\n",
    "            lista_anotacoes_1 = pd.DataFrame(np.array(sorted(documento.anotacoes[id1],key= lambda x: x[0]))).drop_duplicates().values\n",
    "            lista_anotacoes_2 = pd.DataFrame(np.array(sorted(documento.anotacoes[id2],key= lambda x: x[0]))).drop_duplicates().values\n",
    "            matriz_concordancia, tag_anotacao, tag_anotacao_disc,conta_append = listas_anotacoes_para_matriz_concordancia(lista_anotacoes_1,lista_anotacoes_2,legenda)\n",
    "            dict_matriz_doc['matriz_concordancia'] = matriz_concordancia\n",
    "            conta += conta_append\n",
    "            lista_matrizes.append(dict_matriz_doc)\n",
    "            dict_doc      = {}\n",
    "            dict_doc_disc = {}\n",
    "            \n",
    "            # cria uma lista com as anotações de todos os documentos\n",
    "            ct_tag += len(tag_anotacao)\n",
    "            for item in tag_anotacao:\n",
    "                lista_cdjur.append(item)\n",
    "                if item[0] == 'CLASSECNJ' or item[0] == 'ASSUNTOCNJ':\n",
    "                    pass\n",
    "                else:\n",
    "                    dict_doc[item[0]] = {'inicio': item[1][0], 'fim': item[1][1], 'texto': item[1][2]}\n",
    "                    lista_dict.append(dict_doc)\n",
    "                    dict_doc = {}\n",
    "\n",
    "            # cria uma lista com as anotações de todos os documentos - discordantes\n",
    "            for item in tag_anotacao_disc:\n",
    "                lista_cdjur_disc.append(item)\n",
    "                if item[0] == 'CLASSECNJ' or item[0] == 'ASSUNTOCNJ':\n",
    "                    pass\n",
    "                else:\n",
    "                    dict_doc_disc[item[0]] = {'inicio': item[1][0], 'fim': item[1][1], 'texto': item[1][2]}\n",
    "                    lista_dict_disc.append(dict_doc_disc)\n",
    "                    dict_doc_disc = {}\n",
    "\n",
    "        # Cria o dicionário final da cdjur por documento\n",
    "        dicionario_cdjur[documento.cddocumento] = lista_dict\n",
    "        lista_dict      = []\n",
    "        # Cria o dicionário final da cdjur por documento - discordante\n",
    "        dicionario_cdjur_disc[documento.cddocumento] = lista_dict_disc\n",
    "        lista_dict_disc = []\n",
    "        \n",
    "    print(\"conta lista_anotacao\", ct_tag)\n",
    "    print(\"conta appends\", conta)\n",
    "    print('Quantidade de anotações:', len(lista_cdjur))\n",
    "    print('Quantidade de anotações discordantes:', len(lista_cdjur_disc))\n",
    "    \n",
    "    # Retira as repetições de anotações (deixa lista única)\n",
    "    lst_cdjur = list(set(lista_cdjur))\n",
    "    \n",
    "    # Cria dicionário para a CDJUR\n",
    "    # Seleciona as tags para usar como chaves no dicionário\n",
    "    chaves = []\n",
    "    for idx,tupla in enumerate(lst_cdjur):\n",
    "        chaves.append(tupla[0])\n",
    "\n",
    "    dict_cdjur = dict.fromkeys(list(set(chaves)), [])\n",
    "\n",
    "    for tag in dict_cdjur.keys():\n",
    "        lista_aux = []\n",
    "            \n",
    "        for tag_ann in lista_cdjur:\n",
    "            if tag_ann[0] == tag:\n",
    "                lista_aux.append(tag_ann[1][2])\n",
    "\n",
    "        dict_cdjur[tag] = lista_aux\n",
    "\n",
    "    with open('cdjur'+sufixo_file+'.json', 'w', encoding='utf-8') as file:\n",
    "        json.dump(dict_cdjur, file, ensure_ascii=False)\n",
    "    \n",
    "###### para gerar lista de tags discordantes\n",
    "\n",
    "    # Retira as repetições de anotações (deixa lista única)                 \n",
    "    lst_cdjur_disc = list(set(lista_cdjur_disc))\n",
    "    \n",
    "    # Cria dicionário para a CDJUR\n",
    "    # Seleciona as tags para usar como chaves no dicionário\n",
    "    chaves = []\n",
    "    for idx,tupla in enumerate(lst_cdjur_disc):\n",
    "        chaves.append(tupla[0])\n",
    "\n",
    "    dict_cdjur_disc = dict.fromkeys(list(set(chaves)), [])\n",
    "\n",
    "    for tag in dict_cdjur_disc.keys():\n",
    "        lista_aux = []\n",
    "            \n",
    "        for tag_ann in lista_cdjur_disc:\n",
    "            if tag_ann[0] == tag:\n",
    "                lista_aux.append(tag_ann[1][2])\n",
    "\n",
    "        dict_cdjur_disc[tag] = lista_aux\n",
    "\n",
    "    with open('cdjur_disc'+sufixo_file+'.json', 'w', encoding='utf-8') as file:\n",
    "        json.dump(dict_cdjur_disc, file, ensure_ascii=False)\n",
    "######\n",
    "        \n",
    "#   grava arquivo final da cdjur por documento\n",
    "    with open('cdjur_por_documento'+sufixo_file+'.json', 'w', encoding='utf-8') as file:\n",
    "        json.dump(dicionario_cdjur, file, ensure_ascii=False)\n",
    "\n",
    "#   grava arquivo final da cdjur por documento - discordantes\n",
    "    with open('cdjur_por_documento_disc'+sufixo_file+'.json', 'w', encoding='utf-8') as file:\n",
    "        json.dump(dicionario_cdjur_disc, file, ensure_ascii=False)\n",
    "    \n",
    "    dataframe_documentos = pd.DataFrame(lista_matrizes)\n",
    "    \n",
    "    # Gera Dataframe da Coleção Dourada e grava em arquivo .csv\n",
    "#     df_cdjur = dataframe_documentos[['cddocumento', 'tag_anotacao']]\n",
    "#     df_cdjur.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "#     df_cdjur.to_csv('cdjur'+sufixo_file+'.csv', sep=',', encoding='utf-8')\n",
    "\n",
    "    return dataframe_documentos\n",
    "\n",
    "# ETAPA FINAL - Gera as planilhas com os valores Kappa                                                 ETAPA FINAL\n",
    "def dataframe_documentos_para_xlsx(dataframe_documentos, dict_entidades, caminho_xls):\n",
    "    print('Iniciando última etapa.')\n",
    "    dataframe_documentos['kappa_geral'] = dataframe_documentos['matriz_concordancia'].apply(medida_kappa)\n",
    "    for categoria, tags in tqdm(dict_entidades.items()):\n",
    "        dataframe_documentos['kappa_'+categoria.lower()] = dataframe_documentos['matriz_concordancia'].apply(lambda x: get_submatriz_concordancia(x,tags)).apply(medida_kappa)\n",
    "    \n",
    "    dataframe_documentos.to_csv('df_doc_completo'+sufixo_file+'.csv', sep=',', encoding='utf-8') # Provisório\n",
    "    writer = pd.ExcelWriter(caminho_xls, engine='xlsxwriter')\n",
    "    dataframe_documentos.drop('matriz_concordancia',axis=1).to_excel(writer,sheet_name= 'resumo_kappa',index=True,startcol=0,startrow=0)\n",
    "\n",
    "    for linha in tqdm(dataframe_documentos.itertuples(index=True)):\n",
    "        startcol=0\n",
    "        startrow=0\n",
    "        sheet_name = linha.Index\n",
    "        matriz_concordancia = linha.matriz_concordancia\n",
    "        matriz_concordancia.replace(0,\"-\").to_excel(writer,sheet_name= str(sheet_name),index=True,startcol=startcol,startrow=startrow)\n",
    "        startrow += matriz_concordancia.shape[0] + 2\n",
    "        for categoria, tags in dict_entidades.items():\n",
    "            m = get_submatriz_concordancia(matriz_concordancia,tags)\n",
    "            m.replace(0,\"-\").to_excel(writer,sheet_name= str(sheet_name),index=True,startcol=startcol,startrow=startrow)\n",
    "            startrow += m.shape[0] + 2\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "#                                       MÓDULO INICIAL DE CÁLCULO DAS MÉTRICAS\n",
    "if __name__ == \"__main__\":\n",
    "#     del str\n",
    "    start_time = time.time()\n",
    "    threshold = 0.1\n",
    "    funcao_similaridade = get_funcao_similaridade(threshold, similaridade_jaccard,similaridade_esta_contido)\n",
    "    \n",
    "#     legenda_cd = {'e_71': 'PES_PROMOTOR_MP', 'e_60': 'PROVA_ACAREAÇÃO', 'e_16': 'PES_REU_JUR', 'e_35': 'PENA_RESTRITIVA_DIREITO',\n",
    "#                   'e_64': 'PROVA_DECL_VITIMA', 'e_38': 'END_REU', 'e_53': 'PROVA_INSTRUM_CRIME', 'e_34': 'PENA_MUL',\n",
    "#                   'e_56': 'PROVA_CONFISSÃO', 'e_67': 'END_OUTROS', 'e_30': 'SENTENÇA_SEM_RESOL', 'e_12': 'PES_AUTOR_JUR',\n",
    "#                   'e_23': 'PES_TESTEMUNHA', 'e_70': 'END_VITIMA', 'e_52': 'PROVA_LOCAL_CRIME', 'e_63': 'PROVA_DISPMOVEIS',\n",
    "#                   'e_31': 'SENTENÇA_ACORDO', 'e_20': 'PES_ADVOG_AUT', 'e_18': 'PES_JUIZ', 'e_29': 'SENTENÇA_IMPROCED',\n",
    "#                   'e_57': 'PROVA_TESTEM_ACUS', 'e_11': 'PES_AUTOR_FIS', 'e_58': 'PROVA_TESTEM_DEFESA', 'e_65': 'PROVA_OBJETO_APREENDIDO',\n",
    "#                   'e_33': 'PENA_INDENIZACAO', 'e_54': 'PROVA_EXAME_LAB', 'e_48': 'PROVA_CORPO_DELITO', 'e_19': 'PES_JURI',\n",
    "#                   'e_51': 'PROVA_EXUMAÇÃO', 'e_37': 'END_AUTOR', 'e_62': 'PROVA_REPRODUÇÃO', 'm_69': 'ASSUNTO_CNJ',\n",
    "#                   'e_73': 'SENTENÇA_PROCED_PARCIAL', 'e_26': 'PES_OUTROS', 'e_39': 'END_DELITO', 'e_40': 'END_TESTEMUNHA', 'e_21': 'PES_ADVOG_REU',\n",
    "#                   'e_59': 'PROVA_RECONHECIMENTO', 'e_28': 'SENTENÇA_PROCED', 'e_17': 'PES_VITIMA', 'e_9': 'NOR_PRINCIPAL',\n",
    "#                   'e_10': 'NOR_ACESSORIA', 'e_55': 'PROVA_INTERROG', 'm_68': 'CLASSE_CNJ', 'e_32': 'DEC_DECISÃO_INTERL',\n",
    "#                   'e_61': 'PROVA_DOCUMENTAL', 'e_36': 'PENA_PRISAO', 'e_50': 'PROVA_AUTOPSIA', 'e_25': 'PES_AUTORID_POLICIAL',\n",
    "#                   'e_24': 'PES_MP_CUSTOS_IURIS', 'e_13': 'PES_REU_FIS', 'e_72': 'NOR_JURISPRUDÊNCIA'}\n",
    "#     print('Gerando Coleção Dourada do Ministério Público.')\n",
    "#     caminho_cdjur_mp = \"C://Users//mbrit/tagtog_CDJUR-MP/CDJUR-MP/ann.json/master\"\n",
    "#     caminho_cdjur_mp = \"C://Users//mbrit/tagtog_CDJUR-MP/CDJUR-MP/ann.json/members\"\n",
    "#     caminho_legenda_mp = \"C://Users//mbrit/tagtog_CDJUR-MP/CDJUR-MP/annotations-legend.json\"\n",
    "#     sufixo_file = '_rev_mp'\n",
    "#     legenda_cd = json.load(open(caminho_legenda_mp,'rb'))\n",
    "#     legenda_cd = {key: value for key, value in legenda_cd.items() if key.startswith(\"e_\") or key.startswith(\"m_\")}\n",
    "    \n",
    "#     dict_entidades = get_dict_classe_tag(legenda_cd)\n",
    "#     dataframe_documentos = caminho_anotacoes_para_df(caminho_cdjur_mp,legenda_cd)    \n",
    "#     dataframe_documentos_para_xlsx(dataframe_documentos, dict_entidades, \"metrica\"+sufixo_file+\".xlsx\")\n",
    "\n",
    "#     Início TJ\n",
    "    print('Gerando Coleção Dourada do Tribunal de Justiça.')\n",
    "    caminho_cdjur_tj = \"C://Users//mbrit/tagtog_CDJUR/CDJUR/ann.json/master\"\n",
    "#     caminho_cdjur_tj = \"C://Users//mbrit/tagtog_CDJUR/CDJUR/ann.json/members\" # original\n",
    "    caminho_legenda_tj = \"C://Users//mbrit/tagtog_CDJUR/CDJUR/annotations-legend.json\"\n",
    "    sufixo_file = '_master_tj'\n",
    "    \n",
    "    legenda_cd = json.load(open(caminho_legenda_tj,'rb'))\n",
    "    \n",
    "    legenda_cd = {key: value for key, value in legenda_cd.items() if key.startswith(\"e_\") or key.startswith(\"m_\")}\n",
    "    \n",
    "    dict_entidades = get_dict_classe_tag(legenda_cd)\n",
    "\n",
    "    dataframe_documentos = caminho_anotacoes_para_df(caminho_cdjur_tj,legenda_cd)\n",
    "#     dataframe_documentos_para_xlsx(dataframe_documentos, dict_entidades, \"metrica\"+sufixo_file+\".xlsx\")\n",
    "    \n",
    "    import winsound\n",
    "    frequency = 2500  # Set Frequency To 2500 Hertz\n",
    "    duration = 500  # Set Duration To 1000 ms == 1 second\n",
    "    winsound.Beep(frequency, duration)\n",
    "    duration = 900  # Set Duration To 1000 ms == 1 second\n",
    "    winsound.Beep(frequency, duration)\n",
    "    print(\"--- %s segundos ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f12585d",
   "metadata": {},
   "source": [
    "### Módulo de Estatísticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e641ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e3e41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_com = pd.read_csv('df_doc_completo_mp.csv')\n",
    "df_com.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b31cb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_com.loc[594,\"matriz_concordancia\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a27230",
   "metadata": {},
   "source": [
    "### Carrega Dataset de documentos e anotações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4c8b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_docs = pd.read_csv('df_docs_int_mp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c81b2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_docs.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3e217c",
   "metadata": {},
   "source": [
    "### Pré-processamento para gerar estatísticas das tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81db3fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_tag = ['CLASSE_CNJ', 'ASSUNTO_CNJ', 'NOR_PRINCIPAL', 'NOR_ACESSORIA', 'NOR_JURISPRUDÊNCIA', 'PES_VITIMA', 'PES_AUTOR_FIS', 'PES_AUTOR_JUR', 'PES_REU_FIS', 'PES_REU_JUR', 'PES_JUIZ',\n",
    "'PES_JURI', 'PES_ADVOG_AUT', 'PES_ADVOG_REU', 'PES_TESTEMUNHA', 'PES_MP_CUSTOS_IURIS', 'PES_AUTORID_POLICIAL', 'PES_OUTROS',\n",
    "'PES_PROMOTOR_MP', 'SENTENÇA_PROCED', 'SENTENÇA_IMPROCED', 'SENTENÇA_PROCED_PARCIAL', 'SENTENÇA_SEM_RESOL', 'SENTENÇA_ACORDO', 'DEC_DECISÃO_INTERL', 'PENA_INDENIZACAO',\n",
    "'PENA_MUL', 'PENA_RESTRITIVA_DIREITO', 'PENA_PRISAO', 'END_AUTOR', 'END_REU', 'END_DELITO', 'END_TESTEMUNHA','END_OUTROS',\n",
    "'END_VITIMA', 'PROVA_CORPO_DELITO', 'PROVA_AUTOPSIA', 'PROVA_EXUMAÇÃO', 'PROVA_LOCAL_CRIME', 'PROVA_INSTRUM_CRIME', 'PROVA_EXAME_LAB',\n",
    "'PROVA_INTERROG', 'PROVA_CONFISSÃO', 'PROVA_TESTEM_ACUS', 'PROVA_TESTEM_DEFESA', 'PROVA_RECONHECIMENTO', 'PROVA_ACAREAÇÃO',\n",
    "'PROVA_DOCUMENTAL', 'PROVA_REPRODUÇÃO', 'PROVA_DISPMOVEIS', 'PROVA_DECL_VITIMA', 'PROVA_OBJETO_APREENDIDO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fb511b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "lista_tag = [] # lista geral de ocorrências de tags em todos os documentos\n",
    "# set_tag = []\n",
    "# df_estat_tag = pd.DataFrame()\n",
    "for lista in df_docs[\"anotacoes\"].values:\n",
    "    tokens = re.split(r'\\W+', lista)\n",
    "    lista_doc_tag = [] # Lista de tags por documento\n",
    "    for tk in tokens:\n",
    "        if tk in set_tag:\n",
    "            tag = tk.split()\n",
    "            try:\n",
    "#             if not tag[0].isdigit():\n",
    "                lista_doc_tag.append(tag[0])\n",
    "#                 set_tag.append(tag[0])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    lista_tag.append(lista_doc_tag)\n",
    "\n",
    "# set_tag = list(set(set_tag)) # Conjunto de tags (únicas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b1d336",
   "metadata": {},
   "source": [
    "### Cria Dataset de estatística das tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9aca0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colunas do novo dataset\n",
    "columns_nm = ['cddocumento','tags'] + set_tag\n",
    "df_estat_tag = pd.DataFrame(columns=columns_nm)\n",
    "df_estat_tag['cddocumento'] = df_docs['cddocumento']\n",
    "df_estat_tag['tags'] = lista_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c3ecd2",
   "metadata": {},
   "source": [
    "### Totaliza ocorrência de tags por documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afb41a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "estatistica = {}\n",
    "for tag in set_tag:\n",
    "    estatistica[tag] = 0\n",
    "for i, lista in enumerate(df_estat_tag['tags'].values):\n",
    "    for tag in lista:\n",
    "        estatistica[tag] = estatistica[tag] + 1\n",
    "    for tag in set_tag:\n",
    "        df_estat_tag.loc[i,tag] = estatistica[tag]\n",
    "        estatistica[tag] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289ef099",
   "metadata": {},
   "source": [
    "### Total de anotações por tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87b56f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenha um novo conjunto com elementos que estão apenas em a, mas não em b\n",
    "cols = []\n",
    "cols = [x for x in set_tag] # if (x not in ['PES_TESTEMUNHA_AUT'])]\n",
    "# Grava csv\n",
    "df_estat_tag[cols].sum().sort_values().to_csv('sumário_tags_mp.csv', sep=',', encoding='utf-8')\n",
    "df_estat_tag[cols].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b2fe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estat_tag[cols].sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b33e504",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,14))\n",
    "df_estat_tag[cols].sum().plot.barh(title=\"Quantidade de Anotações por Tag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22876916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gera dataset com colunas selecionadas\n",
    "cols = ['cddocumento'] + cols\n",
    "df = df_estat_tag[cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2696680",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e95a69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dc7e0e",
   "metadata": {},
   "source": [
    "### Grava arquivo de estatístivas das tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fb687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grava arquivo de estatísticas\n",
    "df.to_csv('estatisticas_tags_mp.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2858b76d",
   "metadata": {},
   "source": [
    "### Abre a Coleção Dourada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71559f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cdjur = pd.read_csv('cdjur.csv')\n",
    "df_cdjur.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f7ac4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cdjur.drop(['Unnamed: 0'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4d011a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 800)\n",
    "df_cdjur.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2417cda1",
   "metadata": {},
   "source": [
    "### Abre CDJUR JSON - Discordantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce279a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abre CDJUR do TJ + Revisada\n",
    "cdjur_an_tj = open('cdjur_disc_an_tj.json', \"r\")\n",
    "cdjur_rev_tj = open('cdjur_disc_rev_tj.json', \"r\")\n",
    "\n",
    "# Abre CDJUR do MP + Revisada\n",
    "cdjur_an_mp = open('cdjur_disc_an_mp.json', \"r\")\n",
    "cdjur_rev_mp = open('cdjur_disc_rev_mp.json', \"r\")\n",
    "\n",
    "# Retorna CDJUR do TJ como Dicionário do Python\n",
    "dict_cdjur_an_tj = json.load(cdjur_an_tj)\n",
    "dict_cdjur_rev_tj = json.load(cdjur_rev_tj)\n",
    "\n",
    "# Retorna CDJUR do MP como Dicionário do Python\n",
    "dict_cdjur_an_mp = json.load(cdjur_an_mp)\n",
    "dict_cdjur_rev_mp = json.load(cdjur_rev_mp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85276cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TJ Anotações discordantes\n",
    "a = 0 \n",
    "for tag in dict_cdjur_an_tj.keys():\n",
    "    a += len(dict_cdjur_an_tj[tag])\n",
    "    \n",
    "# TJ Revisões discordantes\n",
    "b = 0\n",
    "for tag in dict_cdjur_rev_tj.keys():\n",
    "    b += len(dict_cdjur_rev_tj[tag])\n",
    "    \n",
    "# MP Anotações discordantes\n",
    "c = 0\n",
    "for tag in dict_cdjur_an_mp.keys():\n",
    "    c += len(dict_cdjur_an_mp[tag])\n",
    "    \n",
    "# MP Revisões discordantes\n",
    "d = 0\n",
    "for tag in dict_cdjur_rev_mp.keys():\n",
    "    d += len(dict_cdjur_rev_mp[tag])\n",
    "\n",
    "print(f'TJ - Tags:{len(dict_cdjur_an_tj)}, Anotações:{a}')\n",
    "print(f'TJ - Tags:{len(dict_cdjur_rev_tj)}, Anotações:{b}')\n",
    "print(f'Total TJ: {a+b}')\n",
    "print(f'MP - Tags:{len(dict_cdjur_an_mp)}, Anotações:{c}')\n",
    "print(f'MP - Tags:{len(dict_cdjur_rev_mp)}, Anotações:{d}')\n",
    "print(f'Total MP: {c+d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fc5a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclue tags inválidas\n",
    "# cdjur_mp\n",
    "del dict_cdjur_an_mp['CLASSECNJ']\n",
    "del dict_cdjur_an_mp['ASSUNTOCNJ']\n",
    "# cdjur_rev_mp\n",
    "del dict_cdjur_rev_mp['CLASSECNJ']\n",
    "del dict_cdjur_rev_mp['ASSUNTOCNJ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89238fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_cdjur_an_tj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d337ccaa",
   "metadata": {},
   "source": [
    "### Juntas as Coleções Douradas - Interseção"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4283ccf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# junta as coleçções do TJ\n",
    "dict_cdjur1 = dict_cdjur_int_tj\n",
    "for i, tag2 in enumerate(dict_cdjur_rev_int_tj):\n",
    "    if tag2 in dict_cdjur_int_tj.keys():\n",
    "        dict_cdjur1[tag2] = list(set(dict_cdjur_int_tj[tag2]+dict_cdjur_rev_int_tj[tag2]))\n",
    "    else:\n",
    "        dict_cdjur1[tag2] = dict_cdjur_rev_int_tj[tag2]\n",
    "# junta as coleçções do MP\n",
    "dict_cdjur2 = dict_cdjur_int_mp\n",
    "for i, tag2 in enumerate(dict_cdjur_rev_int_mp):\n",
    "    if tag2 in dict_cdjur_int_mp.keys():\n",
    "        dict_cdjur2[tag2] = list(set(dict_cdjur_int_mp[tag2]+dict_cdjur_rev_int_mp[tag2]))\n",
    "    else:\n",
    "        dict_cdjur2[tag2] = dict_cdjur_rev_int_mp[tag2]\n",
    "# junta as coleções do TJ e MP\n",
    "dict_cdjur_int = dict_cdjur1\n",
    "for i, tag2 in enumerate(dict_cdjur2):\n",
    "    if tag2 in dict_cdjur1.keys():\n",
    "        dict_cdjur_int[tag2] = list(set(dict_cdjur1[tag2]+dict_cdjur2[tag2]))\n",
    "    else:\n",
    "        dict_cdjur_int[tag2] = dict_cdjur2[tag2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677aa932",
   "metadata": {},
   "source": [
    "#### Grava Coleção Dourada Única"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c714b137",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cdjur_int.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(dict_cdjur_int, file)\n",
    "\n",
    "# Fecha os arquivos\n",
    "cdjur_int_tj.close()\n",
    "cdjur_rev_int_tj.close()\n",
    "cdjur_int_mp.close()\n",
    "cdjur_rev_int_mp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ccce51",
   "metadata": {},
   "source": [
    "#### Visualização da CDJUR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca69dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dict_cdjur_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c2069e",
   "metadata": {},
   "source": [
    "#### Descritivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcef54be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tag:    Quantidade de Anotações:')\n",
    "total = 0\n",
    "quantidades = []\n",
    "for tag in dict_cdjur_int.keys():\n",
    "    print(tag, len(dict_cdjur_int[tag]))\n",
    "    total += len(dict_cdjur_int[tag])\n",
    "    quantidades.append(len(dict_cdjur_int[tag]))\n",
    "print('\\n Total:', total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b317b074",
   "metadata": {},
   "source": [
    "#### Gráfico de Barras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f1c8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = list(dict_cdjur_int.keys())\n",
    "plt.figure(figsize=(10,14))\n",
    "plt.barh(range(len(dict_cdjur_int)), quantidades, tick_label=tags)\n",
    "plt.title(\"Ocorrências por Tag\")\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d225b42",
   "metadata": {},
   "source": [
    "#### Algumas amostras da CDJUR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b12867",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(dict_cdjur_int['SENTENÇA_PROCED_PARCIAL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81311d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_cdjur_int['PES_MP_CUSTOS_IURIS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dfa12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_cdjur_int['PES_JURI']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f0cb53",
   "metadata": {},
   "source": [
    "### Juntas as Coleções Douradas - União"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160561e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# junta as coleções do TJ\n",
    "dict_cdjur1 = dict_cdjur_un_tj\n",
    "for i, tag2 in enumerate(dict_cdjur_rev_un_tj):\n",
    "    if tag2 in dict_cdjur_un_tj.keys():\n",
    "        dict_cdjur1[tag2] = list(set(dict_cdjur_un_tj[tag2]+dict_cdjur_rev_un_tj[tag2]))\n",
    "    else:\n",
    "        dict_cdjur1[tag2] = dict_cdjur_rev_un_tj[tag2]\n",
    "# junta as coleçções do MP\n",
    "dict_cdjur2 = dict_cdjur_un_mp\n",
    "for i, tag2 in enumerate(dict_cdjur_rev_un_mp):\n",
    "    if tag2 in dict_cdjur_un_mp.keys():\n",
    "        dict_cdjur2[tag2] = list(set(dict_cdjur_un_mp[tag2]+dict_cdjur_rev_un_mp[tag2]))\n",
    "    else:\n",
    "        dict_cdjur2[tag2] = dict_cdjur_rev_un_mp[tag2]\n",
    "# junta as coleções do TJ e MP\n",
    "dict_cdjur_un = dict_cdjur1\n",
    "for i, tag2 in enumerate(dict_cdjur2):\n",
    "    if tag2 in dict_cdjur1.keys():\n",
    "        dict_cdjur_un[tag2] = list(set(dict_cdjur1[tag2]+dict_cdjur2[tag2]))\n",
    "    else:\n",
    "        dict_cdjur_un[tag2] = dict_cdjur2[tag2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a536c1",
   "metadata": {},
   "source": [
    "#### Grava CDJUR única - União"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1c9db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cdjur_un.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(dict_cdjur_un, file)\n",
    "\n",
    "# Fecha os arquivos\n",
    "cdjur_un_tj.close()\n",
    "cdjur_rev_un_tj.close()\n",
    "cdjur_un_mp.close()\n",
    "cdjur_rev_un_mp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f298a7ca",
   "metadata": {},
   "source": [
    "#### Visualização da CDJUR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860a493d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dict_cdjur_un)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377fa1f7",
   "metadata": {},
   "source": [
    "#### Descritivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54df4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tag:    Quantidade de Anotações:')\n",
    "total = 0\n",
    "quantidades = []\n",
    "for tag in dict_cdjur_un.keys():\n",
    "    print(tag, len(dict_cdjur_un[tag]))\n",
    "    total += len(dict_cdjur_un[tag])\n",
    "    quantidades.append(len(dict_cdjur_un[tag]))\n",
    "print('\\n Total:', total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac8ef54",
   "metadata": {},
   "source": [
    "#### Gráfico de Barras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76acf6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = list(dict_cdjur_un.keys())\n",
    "plt.figure(figsize=(10,14))\n",
    "plt.barh(range(len(dict_cdjur_un)), quantidades, tick_label=tags)\n",
    "plt.title(\"Ocorrências por Tag\")\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2cfc43",
   "metadata": {},
   "source": [
    "#### Algumas amostras da CDJUR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ef0ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dict_cdjur_un['SENTENÇA_PROCED_PARCIAL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c7afc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_cdjur_un['SENTENÇA_ACORDO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acace64",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_cdjur_un['PROVA_LOCAL_CRIME']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
